{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers datasets scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nfrom datasets import load_dataset\nfrom transformers import (\n    BertTokenizer,\n    BertForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_fscore_support,\n    ConfusionMatrixDisplay\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"Sp1786/multiclass-sentiment-analysis-dataset\")\n\ntrain_data = ds[\"train\"]\ntest_data  = ds[\"test\"]\n\nprint(train_data[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_to_text = {}\n\nfor example in train_data:\n    label_to_text[example[\"label\"]] = example[\"sentiment\"]\n\nprint(label_to_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import Counter\n\nlabels = train_data[\"label\"]\ncounts = Counter(labels)\n\nplt.bar(counts.keys(), counts.values())\nplt.title(\"Class Distribution — Training Set\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained(\n    \"bert-base-uncased\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NewsDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        text = self.texts[idx] if isinstance(self.texts[idx], str) else \"\"\n\n        encoding = tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=256,   # ← recommended\n            return_tensors='pt'\n        )\n\n        item = {k: v.squeeze(0) for k, v in encoding.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = NewsDataset(\n    list(train_data[\"text\"]),\n    list(train_data[\"label\"])\n)\n\ntest_dataset = NewsDataset(\n    list(test_data[\"text\"]),\n    list(test_data[\"label\"])\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nlabels = list(train_data[\"label\"])\n\nclass_weights = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.unique(labels),\n    y=labels\n)\n\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\nprint(\"Class weights:\", class_weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\n\nloss_fn = nn.CrossEntropyLoss(weight=class_weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_labels = len(set(train_data[\"label\"]))\n\nfrom transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=num_labels\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\nepochs = 3\ntotal_steps = len(train_loader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\n\nfor epoch in range(epochs):\n    total_loss = 0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(\n            input_ids,\n            attention_mask=attention_mask\n        )\n\n        loss = loss_fn(outputs.logits, labels)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\npreds = []\ntrue = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        predictions = torch.argmax(logits, dim=1)\n\n        preds.extend(predictions.cpu().numpy())\n        true.extend(labels.cpu().numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nacc = accuracy_score(true, preds)\n\nprecision, recall, f1, _ = precision_recall_fscore_support(\n    true, preds, average=\"weighted\"\n)\n\nprint(\"Accuracy:\", acc)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nConfusionMatrixDisplay.from_predictions(true, preds)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_text(text: str):\n    model.eval()\n\n    inputs = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True\n    ).to(device)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        probs = F.softmax(outputs.logits, dim=1)\n\n    confidence, pred = torch.max(probs, dim=1)\n\n    label_name = label_to_text[pred.item()]\n\n    return label_name, float(confidence.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"examples = [\n    \"I absolutely loved this product!\",\n    \"This was the worst experience ever.\",\n    \"It was okay, nothing special.\",\n    \"Amazing performance and great value.\"\n]\n\nfor text in examples:\n    label, conf = predict_text(text)\n    print(f\"Text: {text}\")\n    print(f\"Predicted Label: {label}, Confidence: {conf:.4f}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}